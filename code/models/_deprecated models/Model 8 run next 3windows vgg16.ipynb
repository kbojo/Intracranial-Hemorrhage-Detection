{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intracranial Hemorrhage Detection\n",
    "\n",
    "## Model 8\n",
    "\n",
    "#### Kristina Joos\n",
    "\n",
    "---   \n",
    "\n",
    "|                 \t|                                                        \t|\n",
    "|:----------------:\t|:-------------------------------------------------------:\t|\n",
    "| Model           \t| VGG 16 (224x224)  Multiple Outputs Heads                                        |\n",
    "| Augementation    \t| Crop, Affine, Flip                \t                    |\n",
    "| Windowing         | Three Windows                                             |\n",
    "| Class Balancing \t| Undersampling                                           \t|\n",
    "| Loss Function   \t| Binary_crossentropy                                      \t|\n",
    "| Regularization  \t| Early Stopping, Drop Out Layers 0.3 \t                    |\n",
    "| Epochs Run      \t|                                                        \t|\n",
    "| Time Run (min)   \t| start 22:10                                             \t|\n",
    "|                 \t|                                                        \t|\n",
    "| Test Sores      \t| Accuracy: Loss:                                        \t|\n",
    "| Validation      \t| Accuracy: Loss:                                        \t|\n",
    "| Leader Board    \t| Score: Rank:                                           \t|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor, log\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "\n",
    "# from keras_applications.resnet import ResNet50\n",
    "from keras_applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    " \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet_50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg_16\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, VGG19\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet_50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg_16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Activation, concatenate, Dropout, MaxPooling2D, Conv2D, Flatten\n",
    "from tensorflow.keras.initializers import glorot_normal, he_normal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model, load_model, Sequential, load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_data = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_files(path, data_name):\n",
    "    no =  len(os.listdir(path))\n",
    "    print (f'The {data_name} contains {no} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_directory(path, data_name):\n",
    "    size = round(sum([os.path.getsize(f'{path}'+ f'{file}') for file in os.listdir(path)])*(10**-9), 2)\n",
    "    print (f'The size of the {data_name} is {size} GB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAIN DATA***\n",
      "The size of the Training Data is 395.19 GB.\n",
      "The Training Data contains 752803 files.\n",
      "*******\n",
      "***TEST DATA***\n",
      "The size of the Testing Data is 63.64 GB.\n",
      "The Testing Data contains 121232 files.\n",
      "*******\n",
      "We have 6.2 times more train than test data.\n"
     ]
    }
   ],
   "source": [
    "print('***TRAIN DATA***')\n",
    "get_size_directory(path_train_data, 'Training Data')\n",
    "get_number_of_files(path_train_data, 'Training Data')\n",
    "\n",
    "print('*******')\n",
    "\n",
    "print('***TEST DATA***')\n",
    "get_size_directory(path_test_data, 'Testing Data')\n",
    "get_number_of_files(path_test_data, 'Testing Data')\n",
    "\n",
    "print('*******')\n",
    "\n",
    "print(f'We have {round(len(os.listdir(path_train_data))/len(os.listdir(path_test_data)),1)} times more train than test data.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_meta_df = pd.read_csv('../data/input/rsna-intracranial-hemorrhage-detection/stage_2_train.csv')\n",
    "# Test submission as test \n",
    "test_meta_df = pd.read_csv('../data/input/rsna-intracranial-hemorrhage-detection/stage_2_sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df_shape = train_meta_df.shape\n",
    "test_meta_df_shape = test_meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4516842, 2)\n",
      "(727392, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_meta_df_shape)\n",
    "print(test_meta_df_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seed\n",
    "SEED = 11\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "HEIGHT = 224 #VGG 16 has 256x256\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 4\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Folders\n",
    "#DATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/'\n",
    "PATH_TEST_DATA = path_test_data\n",
    "PATH_TRAIN_DATA = path_train_data\n",
    "PATH_PT_MODELS = '../models/predtrained models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "    return dcm\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "    return bsb_img\n",
    "\n",
    "def _read(path, SHAPE):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(SHAPE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Preprocessor:    \n",
    "    \n",
    "    def __init__(self, path,hu_min_value, hu_max_value, augment=False):\n",
    "        self.path = path\n",
    "        self.nn_input_shape = (224,224,3) #was 256, 256\n",
    "        self.hu_min_value = hu_min_value\n",
    "        self.hu_max_value = hu_max_value\n",
    "        self.augment = augment\n",
    "        \n",
    "  #load the dicom dataset\n",
    "    def load_dicom_dataset(self, filename):\n",
    "        dataset = pydicom.dcmread(self.path + filename)\n",
    "        return dataset\n",
    "    \n",
    "    #Rescale the pixelarray to HU units and set to window\n",
    "    \n",
    "    def get_hounsfield_window(self, dataset):\n",
    "        \n",
    "        windowed_image = bsb_window(dataset)\n",
    " \n",
    "        return windowed_image\n",
    " \n",
    "    def resize(self, image):\n",
    "        image = resize(image, self.nn_input_shape)\n",
    "        return image\n",
    "\n",
    "    #augment our image\n",
    "    def augment_img(self, image): \n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.Crop(keep_size=True, percent=(0.01, 0.05), sample_independently=False),\n",
    "            iaa.Affine(rotate=(-10, 10)),\n",
    "            iaa.Fliplr(0.5)])\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    \n",
    "   \n",
    "    def preprocess(self, identifier):\n",
    "        filename = identifier +  \".dcm\"\n",
    "        dataset = self.load_dicom_dataset(filename)\n",
    "        windowed_image = self.get_hounsfield_window(dataset)\n",
    "        image = self.resize(windowed_image)\n",
    "        if self.augment:\n",
    "            image = self.augment_img(image)\n",
    "        return image\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Generator(Sequence):\n",
    "    \n",
    "    def __init__(self, dataframe,\n",
    "                 preprocessor,\n",
    "                 batch_size,\n",
    "                 shuffle,\n",
    "                 num_classes=5,\n",
    "                 steps=None):\n",
    "        \n",
    "        self.preprocessor = preprocessor\n",
    "        self.data_ids = dataframe.index.values\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.input_shape = (224,224)\n",
    "        self.preprocess_net = preprocess_vgg_16\n",
    "        self.num_classes = num_classes\n",
    "        self.current_epoch=0\n",
    "        \n",
    "        self.steps=steps\n",
    "        if self.steps is not None:\n",
    "            self.steps = np.round(self.steps/3) * 3\n",
    "            self.undersample()\n",
    "\n",
    "    def undersample(self):\n",
    "        part = np.int(self.steps/4 * self.batch_size)\n",
    "        zero_ids = np.random.choice(self.dataframe.loc[self.dataframe[\"any\"] == 0].index.values, size=2*part, replace=False)\n",
    "        hot_ids = np.random.choice(self.dataframe.loc[self.dataframe[\"any\"] == 1].index.values, size=2*part, replace=False)\n",
    "        self.data_ids = list(set(zero_ids).union(hot_ids))\n",
    "        np.random.shuffle(self.data_ids)\n",
    "\n",
    "    # defines the number of steps per epoch\n",
    "    def __len__(self):\n",
    "        if self.steps is None:\n",
    "            return np.int(np.ceil(len(self.data_ids) / np.float(self.batch_size)))\n",
    "        else:\n",
    "            return 3*np.int(self.steps/3) \n",
    "    \n",
    "    # at the end of an epoch: \n",
    "    def on_epoch_end(self):\n",
    "        # if steps is None and shuffle is true:\n",
    "        if self.steps is None:\n",
    "            self.data_ids = self.dataframe.index.values\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.data_ids)\n",
    "        else:\n",
    "            self.undersample()\n",
    "        self.current_epoch += 1\n",
    "    \n",
    "    # should return a batch of images\n",
    "    def __getitem__(self, item):\n",
    "        # select the ids of the current batch\n",
    "        current_ids = self.data_ids[item*self.batch_size:(item+1)*self.batch_size]\n",
    "        X, y_any, y_subtype = self.__generate_batch(current_ids)\n",
    "        return X, [y_any, y_subtype]\n",
    "    \n",
    "    \n",
    "    def __generate_batch(self, current_ids):\n",
    "        X = np.empty((self.batch_size, *self.input_shape, 3))\n",
    "        y_subtype = np.empty((self.batch_size, self.num_classes))\n",
    "        y_any = np.empty((self.batch_size, 1))\n",
    "        for idx, ident in enumerate(current_ids):\n",
    "            # Store sample\n",
    "            image = self.preprocessor.preprocess(ident)\n",
    "            X[idx] = image\n",
    "            # Store class\n",
    "            y_any[idx], y_subtype[idx] = self.__get_target(ident)\n",
    "        return X, y_any, y_subtype\n",
    "    \n",
    "    # extract the targets of one image id:\n",
    "    def __get_target(self, ident):\n",
    "        y_any = self.dataframe.loc[ident, \"any\"]\n",
    "        y_subtype = self.dataframe.drop(\"any\", axis=1).loc[ident].values\n",
    "        return y_any, y_subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Meta Data Frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_usable_df(df):\n",
    "    label = df.Label.values\n",
    "    new_df = df.ID.str.rsplit(\"_\", n=1, expand=True)\n",
    "    new_df.loc[:, \"label\"] = label\n",
    "    new_df = new_df.rename({0: \"id\", 1: \"subtype\"}, axis=1)\n",
    "    piv_df = pd.pivot_table(new_df, index=\"id\", columns=\"subtype\", values=\"label\")\n",
    "    \n",
    "    return piv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Dupicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df_shape[0] - train_meta_df.shape[0] == 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make usable Data Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subtype</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000012eaf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000039fa0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00005679d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00008ce3c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_0000950d7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subtype       any  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "id                                                                              \n",
       "ID_000012eaf    0         0                 0                 0             0   \n",
       "ID_000039fa0    0         0                 0                 0             0   \n",
       "ID_00005679d    0         0                 0                 0             0   \n",
       "ID_00008ce3c    0         0                 0                 0             0   \n",
       "ID_0000950d7    0         0                 0                 0             0   \n",
       "\n",
       "subtype       subdural  \n",
       "id                      \n",
       "ID_000012eaf         0  \n",
       "ID_000039fa0         0  \n",
       "ID_00005679d         0  \n",
       "ID_00008ce3c         0  \n",
       "ID_0000950d7         0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = make_usable_df(train_meta_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check for Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make usable Data Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subtype</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000000e27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000009146</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00007b8cb</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000134952</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000176f2a</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subtype       any  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "id                                                                              \n",
       "ID_000000e27  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000009146  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_00007b8cb  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000134952  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000176f2a  0.5       0.5               0.5               0.5           0.5   \n",
       "\n",
       "subtype       subdural  \n",
       "id                      \n",
       "ID_000000e27       0.5  \n",
       "ID_000009146       0.5  \n",
       "ID_00007b8cb       0.5  \n",
       "ID_000134952       0.5  \n",
       "ID_000176f2a       0.5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = make_usable_df(test_meta_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label Stratified Shuffel Splitter \n",
    "Cross Validaor with stratification on multiple labels:\n",
    "https://github.com/trent-b/iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\n",
    "X = train_df.index\n",
    "Y = train_df[['any', 'epidural', 'intraparenchymal', 'intraventricular',\n",
    "       'subarachnoid', 'subdural']].values\n",
    "\n",
    "# Get train and test index\n",
    "msss_splits = next(msss.split(X, Y))\n",
    "train_idx = msss_splits[0]\n",
    "val_idx = msss_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602242,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150561,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602242, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = train_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150561, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * K.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * K.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_16():\n",
    "    weights_path = '../models/predtrained models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    net = VGG16(include_top=False, weights=weights_path)\n",
    "    for layer in net.layers:\n",
    "        layer.trainable = False\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vgg_16 = vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeModel:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pt_model,\n",
    "                 #loss,\n",
    "                 metrics_list,\n",
    "                 data_generator_train,\n",
    "                 data_generator_val,\n",
    "                 epochs,\n",
    "                 num_classes=6,\n",
    "                 checkpoint_path='../models/mymodels/'):\n",
    "        \n",
    "        self.pt_model = pt_model\n",
    "        #self.loss = loss\n",
    "        self.metrics_list = metrics_list\n",
    "        self.data_generator_train = data_generator_train\n",
    "        self.data_generator_val = data_generator_val\n",
    "        self.epochs = epochs\n",
    "        self.num_classes = num_classes\n",
    "        self.checkpoint_path = checkpoint_path \n",
    "        self.checkpoint = ModelCheckpoint(filepath=self.checkpoint_path,\n",
    "                                          mode=\"min\",\n",
    "                                          verbose=1,\n",
    "                                          save_best_only=True,\n",
    "                                          save_weights_only=True)\n",
    "#         self.learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                             factor=0.5,\n",
    "#                                             patience=2,\n",
    "#                                             min_lr=1e-8,\n",
    "#                                             mode=\"min\")\n",
    "        self.earlystopping = EarlyStopping(monitor=\"val_any_predictions_loss\",\n",
    "                                        patience=5,\n",
    "                                        mode=\"min\",\n",
    "                                        restore_best_weights=True)\n",
    "        \n",
    "    def build_model(self):\n",
    "        base_model = self.pt_model()\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        any_logits = Dense(1, kernel_initializer=he_normal(seed=11))(x)\n",
    "        any_pred = Activation(\"sigmoid\", name=\"any_predictions\")(any_logits)\n",
    "        x = concatenate([any_pred, x])\n",
    "        sub_pred = Dense(self.num_classes,\n",
    "                         name=\"subtype_pred\",\n",
    "                         kernel_initializer=he_normal(seed=11),\n",
    "                         activation=\"sigmoid\")(x) \n",
    "        self.model = Model(inputs=base_model.input, outputs=[any_pred, sub_pred])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss=['binary_crossentropy', focal_loss],\n",
    "                           loss_weights = [1., 0.],\n",
    "                           metrics=self.metrics_list)\n",
    "    \n",
    "    def fit_model(self):\n",
    "        return self.model.fit_generator(generator = self.data_generator_train,\n",
    "                    validation_data=self.data_generator_val,\n",
    "                    epochs=self.epochs,\n",
    "                    callbacks=[self.checkpoint,self.earlystopping],\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=-1)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "    \n",
    "    def predict(self, data_generator_test):\n",
    "        predictions = self.model.predict_generator(data_generator_test, use_multiprocessing = True, workers=-1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor = Preprocessor(path=PATH_TRAIN_DATA,\n",
    "                                  hu_min_value=0,\n",
    "                                  hu_max_value=100,\n",
    "                                  augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preprocessor = Preprocessor(path=PATH_TRAIN_DATA,\n",
    "                                  hu_min_value=0,\n",
    "                                  hu_max_value=100,\n",
    "                                  augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = Preprocessor(path=PATH_TEST_DATA,\n",
    "                                hu_min_value=0,\n",
    "                                hu_max_value=100,\n",
    "                                augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = Data_Generator(train_data,\n",
    "                              train_preprocessor,\n",
    "                              32,\n",
    "                              shuffle=True,\n",
    "                              steps=50)\n",
    "\n",
    "data_generator_val = Data_Generator(val_data, \n",
    "                            val_preprocessor,\n",
    "                            32,\n",
    "                            shuffle=True,\n",
    "                            steps=50)\n",
    "\n",
    "data_generator_test= Data_Generator(test_df, \n",
    "                             test_preprocessor,\n",
    "                             16,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtype\n",
       "any                 0.500000\n",
       "epidural            0.017770\n",
       "intraparenchymal    0.150735\n",
       "intraventricular    0.120098\n",
       "subarachnoid        0.171569\n",
       "subdural            0.220588\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[data_generator_train.data_ids].sum() / train_data.loc[data_generator_train.data_ids].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.7087 - any_predictions_loss: 0.7087 - subtype_pred_loss: 1.0081 - any_predictions_binary_crossentropy: 0.7087 - any_predictions_accuracy: 0.4850 - subtype_pred_focal_loss: 1.0081 - subtype_pred_accuracy: 0.3200\n",
      "Epoch 00001: val_loss improved from inf to 0.67310, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 73s 1s/step - loss: 0.7081 - any_predictions_loss: 0.7081 - subtype_pred_loss: 1.0084 - any_predictions_binary_crossentropy: 0.7081 - any_predictions_accuracy: 0.4847 - subtype_pred_focal_loss: 1.0084 - subtype_pred_accuracy: 0.3205 - val_loss: 0.6731 - val_any_predictions_loss: 0.6731 - val_subtype_pred_loss: 0.5269 - val_any_predictions_binary_crossentropy: 0.6731 - val_any_predictions_accuracy: 0.6550 - val_subtype_pred_focal_loss: 0.5269 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 2/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6862 - any_predictions_loss: 0.6862 - subtype_pred_loss: 1.0119 - any_predictions_binary_crossentropy: 0.6862 - any_predictions_accuracy: 0.5369 - subtype_pred_focal_loss: 1.0119 - subtype_pred_accuracy: 0.3000\n",
      "Epoch 00002: val_loss improved from 0.67310 to 0.65468, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 73s 1s/step - loss: 0.6854 - any_predictions_loss: 0.6854 - subtype_pred_loss: 1.0086 - any_predictions_binary_crossentropy: 0.6854 - any_predictions_accuracy: 0.5392 - subtype_pred_focal_loss: 1.0086 - subtype_pred_accuracy: 0.3002 - val_loss: 0.6547 - val_any_predictions_loss: 0.6547 - val_subtype_pred_loss: 0.5271 - val_any_predictions_binary_crossentropy: 0.6547 - val_any_predictions_accuracy: 0.6826 - val_subtype_pred_focal_loss: 0.5271 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 3/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6677 - any_predictions_loss: 0.6677 - subtype_pred_loss: 0.9984 - any_predictions_binary_crossentropy: 0.6677 - any_predictions_accuracy: 0.5950 - subtype_pred_focal_loss: 0.9984 - subtype_pred_accuracy: 0.3094\n",
      "Epoch 00003: val_loss improved from 0.65468 to 0.64100, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 76s 1s/step - loss: 0.6666 - any_predictions_loss: 0.6666 - subtype_pred_loss: 0.9971 - any_predictions_binary_crossentropy: 0.6666 - any_predictions_accuracy: 0.5968 - subtype_pred_focal_loss: 0.9971 - subtype_pred_accuracy: 0.3082 - val_loss: 0.6410 - val_any_predictions_loss: 0.6410 - val_subtype_pred_loss: 0.5271 - val_any_predictions_binary_crossentropy: 0.6410 - val_any_predictions_accuracy: 0.6930 - val_subtype_pred_focal_loss: 0.5271 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 4/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6619 - any_predictions_loss: 0.6619 - subtype_pred_loss: 1.0338 - any_predictions_binary_crossentropy: 0.6619 - any_predictions_accuracy: 0.6000 - subtype_pred_focal_loss: 1.0338 - subtype_pred_accuracy: 0.3275\n",
      "Epoch 00004: val_loss improved from 0.64100 to 0.62892, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 75s 1s/step - loss: 0.6610 - any_predictions_loss: 0.6610 - subtype_pred_loss: 1.0317 - any_predictions_binary_crossentropy: 0.6610 - any_predictions_accuracy: 0.6011 - subtype_pred_focal_loss: 1.0317 - subtype_pred_accuracy: 0.3272 - val_loss: 0.6289 - val_any_predictions_loss: 0.6289 - val_subtype_pred_loss: 0.5270 - val_any_predictions_binary_crossentropy: 0.6289 - val_any_predictions_accuracy: 0.7065 - val_subtype_pred_focal_loss: 0.5270 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 5/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6455 - any_predictions_loss: 0.6455 - subtype_pred_loss: 1.0034 - any_predictions_binary_crossentropy: 0.6455 - any_predictions_accuracy: 0.6319 - subtype_pred_focal_loss: 1.0034 - subtype_pred_accuracy: 0.3119\n",
      "Epoch 00005: val_loss improved from 0.62892 to 0.62088, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 74s 1s/step - loss: 0.6443 - any_predictions_loss: 0.6443 - subtype_pred_loss: 0.9975 - any_predictions_binary_crossentropy: 0.6443 - any_predictions_accuracy: 0.6330 - subtype_pred_focal_loss: 0.9975 - subtype_pred_accuracy: 0.3100 - val_loss: 0.6209 - val_any_predictions_loss: 0.6209 - val_subtype_pred_loss: 0.5271 - val_any_predictions_binary_crossentropy: 0.6209 - val_any_predictions_accuracy: 0.7077 - val_subtype_pred_focal_loss: 0.5271 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 6/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6462 - any_predictions_loss: 0.6462 - subtype_pred_loss: 0.9906 - any_predictions_binary_crossentropy: 0.6462 - any_predictions_accuracy: 0.6350 - subtype_pred_focal_loss: 0.9906 - subtype_pred_accuracy: 0.3031\n",
      "Epoch 00006: val_loss improved from 0.62088 to 0.61480, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 74s 1s/step - loss: 0.6456 - any_predictions_loss: 0.6456 - subtype_pred_loss: 0.9935 - any_predictions_binary_crossentropy: 0.6456 - any_predictions_accuracy: 0.6373 - subtype_pred_focal_loss: 0.9935 - subtype_pred_accuracy: 0.3027 - val_loss: 0.6148 - val_any_predictions_loss: 0.6148 - val_subtype_pred_loss: 0.5272 - val_any_predictions_binary_crossentropy: 0.6148 - val_any_predictions_accuracy: 0.7077 - val_subtype_pred_focal_loss: 0.5272 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 7/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6382 - any_predictions_loss: 0.6382 - subtype_pred_loss: 1.0034 - any_predictions_binary_crossentropy: 0.6382 - any_predictions_accuracy: 0.6438 - subtype_pred_focal_loss: 1.0034 - subtype_pred_accuracy: 0.3069\n",
      "Epoch 00007: val_loss improved from 0.61480 to 0.60790, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 74s 1s/step - loss: 0.6366 - any_predictions_loss: 0.6366 - subtype_pred_loss: 1.0028 - any_predictions_binary_crossentropy: 0.6366 - any_predictions_accuracy: 0.6477 - subtype_pred_focal_loss: 1.0028 - subtype_pred_accuracy: 0.3100 - val_loss: 0.6079 - val_any_predictions_loss: 0.6079 - val_subtype_pred_loss: 0.5271 - val_any_predictions_binary_crossentropy: 0.6079 - val_any_predictions_accuracy: 0.7077 - val_subtype_pred_focal_loss: 0.5271 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 8/50\n",
      "50/51 [============================>.] - ETA: 1s - loss: 0.6342 - any_predictions_loss: 0.6342 - subtype_pred_loss: 1.0309 - any_predictions_binary_crossentropy: 0.6342 - any_predictions_accuracy: 0.6325 - subtype_pred_focal_loss: 1.0309 - subtype_pred_accuracy: 0.2931\n",
      "Epoch 00008: val_loss improved from 0.60790 to 0.60383, saving model to ../models/mymodels/\n",
      "51/51 [==============================] - 74s 1s/step - loss: 0.6327 - any_predictions_loss: 0.6327 - subtype_pred_loss: 1.0316 - any_predictions_binary_crossentropy: 0.6327 - any_predictions_accuracy: 0.6330 - subtype_pred_focal_loss: 1.0316 - subtype_pred_accuracy: 0.2941 - val_loss: 0.6038 - val_any_predictions_loss: 0.6038 - val_subtype_pred_loss: 0.5271 - val_any_predictions_binary_crossentropy: 0.6038 - val_any_predictions_accuracy: 0.7108 - val_subtype_pred_focal_loss: 0.5271 - val_subtype_pred_accuracy: 0.4877\n",
      "Epoch 9/50\n",
      "23/51 [============>.................] - ETA: 29s - loss: 0.6253 - any_predictions_loss: 0.6253 - subtype_pred_loss: 0.9820 - any_predictions_binary_crossentropy: 0.6253 - any_predictions_accuracy: 0.6603 - subtype_pred_focal_loss: 0.9820 - subtype_pred_accuracy: 0.3003"
     ]
    }
   ],
   "source": [
    "model6 = MakeModel(pt_model=my_vgg_16,\n",
    "                      metrics_list={'any_predictions':['binary_crossentropy', 'accuracy'],\n",
    "                                            'subtype_pred': [focal_loss, 'accuracy']},\n",
    "                      data_generator_train = data_generator_train,\n",
    "                      data_generator_val = data_generator_val,\n",
    "                      epochs=50,\n",
    "                      num_classes=5)\n",
    "model6.build_model()\n",
    "model6.compile_model()\n",
    "model6_history = model6.fit_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['any_predictions_loss']\n",
    "val_loss = model3_history.history['val_any_predictions_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Any Label Training and Validation Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['subtype_pred_loss']\n",
    "val_loss = model3_history.history['val_subtype_pred_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Subtype Label Training and Validation Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['subtype_pred_focal_loss']\n",
    "val_loss = model3_history.history['val_subtype_pred_focal_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Any Label Training and Validation Focal Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model3_history.history['subtype_pred_accuracy']\n",
    "val_acc = model3_history.history['val_subtype_pred_accuracy']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_acc', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_acc', color = 'orange')\n",
    "\n",
    "plt.title('Training and Validation Subtype Accuracy by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_any, train_proba_subtype = model3.predict(data_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_any.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_subtype.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.concatenate((train_proba_any,train_proba_subtype),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(meta_data_df, predictions):\n",
    "    df = pd.DataFrame(predictions, columns=meta_data_df.columns, index=meta_data_df.index)\n",
    "    df = df.stack().reset_index()\n",
    "    df.loc[:, \"ID\"] = df.id.str.cat(df.subtype, sep=\"_\")\n",
    "    df = df.drop([\"id\", \"subtype\"], axis=1)\n",
    "    df = df.rename({0: \"Label\"}, axis=1)\n",
    "    df = df[['ID','Label']]\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission_3 = make_df(test_df, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_3.to_csv('../data/output/submissions/submission_model_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
