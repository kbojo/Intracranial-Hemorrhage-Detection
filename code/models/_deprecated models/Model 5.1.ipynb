{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intracranial Hemorrhage Detection\n",
    "\n",
    "## Model 5\n",
    "\n",
    "#### Kristina Joos\n",
    "\n",
    "---   \n",
    "\n",
    "|                 \t|                                                        \t|\n",
    "|:----------------:\t|:-------------------------------------------------------:\t|\n",
    "| Model           \t| VGG 16 (242x242)                                          |\n",
    "| Preprocessing   \t| Augementation Flip, Single Windowing                   \t|\n",
    "| Class Balancing \t| Oversampling                                           \t|\n",
    "| Loss Function   \t| Binary_crossentropy                                      \t|\n",
    "| Regularization  \t| Early Stopping, Drop Out Layers 0.3 \t                    |\n",
    "| Epochs Run      \t| 50                                                     \t|\n",
    "| Time Run (min)   \t| 60                                                       \t|\n",
    "|                 \t|                                                        \t|\n",
    "| Test Sores      \t| Accuracy: Loss:                                        \t|\n",
    "| Validation      \t| Accuracy: Loss:                                        \t|\n",
    "| Leader Board    \t| Score: Rank:                                           \t|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor, log\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "\n",
    "# from keras_applications.resnet import ResNet50\n",
    "from keras_applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    " \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet_50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg_16\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, VGG19\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet_50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg_16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Activation, concatenate, Dropout, MaxPooling2D, Conv2D, Flatten\n",
    "from tensorflow.keras.initializers import glorot_normal, he_normal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model, load_model, Sequential, load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "\n",
    "#from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_data = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_files(path, data_name):\n",
    "    no =  len(os.listdir(path))\n",
    "    print (f'The {data_name} contains {no} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_directory(path, data_name):\n",
    "    size = round(sum([os.path.getsize(f'{path}'+ f'{file}') for file in os.listdir(path)])*(10**-9), 2)\n",
    "    print (f'The size of the {data_name} is {size} GB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TRAIN DATA***\n",
      "The size of the Training Data is 395.19 GB.\n",
      "The Training Data contains 752803 files.\n",
      "*******\n",
      "***TEST DATA***\n",
      "The size of the Testing Data is 63.64 GB.\n",
      "The Testing Data contains 121232 files.\n",
      "*******\n",
      "We have 6.2 times more train than test data.\n"
     ]
    }
   ],
   "source": [
    "print('***TRAIN DATA***')\n",
    "get_size_directory(path_train_data, 'Training Data')\n",
    "get_number_of_files(path_train_data, 'Training Data')\n",
    "\n",
    "print('*******')\n",
    "\n",
    "print('***TEST DATA***')\n",
    "get_size_directory(path_test_data, 'Testing Data')\n",
    "get_number_of_files(path_test_data, 'Testing Data')\n",
    "\n",
    "print('*******')\n",
    "\n",
    "print(f'We have {round(len(os.listdir(path_train_data))/len(os.listdir(path_test_data)),1)} times more train than test data.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_meta_df = pd.read_csv('../data/input/rsna-intracranial-hemorrhage-detection/stage_2_train.csv')\n",
    "# Test submission as test \n",
    "test_meta_df = pd.read_csv('../data/input/rsna-intracranial-hemorrhage-detection/stage_2_sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df_shape = train_meta_df.shape\n",
    "test_meta_df_shape = test_meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4516842, 2)\n",
      "(727392, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_meta_df_shape)\n",
    "print(test_meta_df_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seed\n",
    "SEED = 11\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "HEIGHT = 224 #VGG 16 has 256x256\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 4\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Folders\n",
    "#DATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/'\n",
    "PATH_TEST_DATA = path_test_data\n",
    "PATH_TRAIN_DATA = path_train_data\n",
    "PATH_PT_MODELS = '../models/predtrained models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "    return bsb_img\n",
    "\n",
    "def _read(path, SHAPE):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(SHAPE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
    "augmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n",
    "                                iaa.Flipud(0.10),\n",
    "                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n",
    "                            ], random_order = True)       \n",
    "        \n",
    "# Generators\n",
    "class TrainDataGenerator(Sequence):\n",
    "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = PATH_TRAIN_DATA, augment = False, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.ids = dataset.index\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, Y = self.__data_generation(indices)\n",
    "        return X, Y\n",
    "\n",
    "    def augmentor(self, image):\n",
    "        augment_img = augmentation        \n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            ID = self.ids[index]\n",
    "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
    "            if self.augment:\n",
    "                X[i,] = self.augmentor(image)\n",
    "            else:\n",
    "                X[i,] = image\n",
    "            Y[i,] = self.labels.iloc[index].values        \n",
    "        return X, Y\n",
    "    \n",
    "class TestDataGenerator(Sequence):\n",
    "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = PATH_TEST_DATA, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.ids = dataset.index\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = test_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indices)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "    \n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            ID = self.ids[index]\n",
    "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
    "            X[i,] = image              \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Meta Data Frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_usable_df(df):\n",
    "    label = df.Label.values\n",
    "    new_df = df.ID.str.rsplit(\"_\", n=1, expand=True)\n",
    "    new_df.loc[:, \"label\"] = label\n",
    "    new_df = new_df.rename({0: \"id\", 1: \"subtype\"}, axis=1)\n",
    "    piv_df = pd.pivot_table(new_df, index=\"id\", columns=\"subtype\", values=\"label\")\n",
    "    \n",
    "    return piv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Dupicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df_shape[0] - train_meta_df.shape[0] == 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make usable Data Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subtype</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000012eaf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000039fa0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00005679d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00008ce3c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_0000950d7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subtype       any  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "id                                                                              \n",
       "ID_000012eaf    0         0                 0                 0             0   \n",
       "ID_000039fa0    0         0                 0                 0             0   \n",
       "ID_00005679d    0         0                 0                 0             0   \n",
       "ID_00008ce3c    0         0                 0                 0             0   \n",
       "ID_0000950d7    0         0                 0                 0             0   \n",
       "\n",
       "subtype       subdural  \n",
       "id                      \n",
       "ID_000012eaf         0  \n",
       "ID_000039fa0         0  \n",
       "ID_00005679d         0  \n",
       "ID_00008ce3c         0  \n",
       "ID_0000950d7         0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = make_usable_df(train_meta_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check for Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make usable Data Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subtype</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000000e27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000009146</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00007b8cb</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000134952</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000176f2a</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subtype       any  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "id                                                                              \n",
       "ID_000000e27  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000009146  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_00007b8cb  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000134952  0.5       0.5               0.5               0.5           0.5   \n",
       "ID_000176f2a  0.5       0.5               0.5               0.5           0.5   \n",
       "\n",
       "subtype       subdural  \n",
       "id                      \n",
       "ID_000000e27       0.5  \n",
       "ID_000009146       0.5  \n",
       "ID_00007b8cb       0.5  \n",
       "ID_000134952       0.5  \n",
       "ID_000176f2a       0.5  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = make_usable_df(test_meta_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label Stratified Shuffel Splitter \n",
    "Cross Validaor with stratification on multiple labels:\n",
    "https://github.com/trent-b/iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\n",
    "X = train_df.index\n",
    "Y = train_df[['any', 'epidural', 'intraparenchymal', 'intraventricular',\n",
    "       'subarachnoid', 'subdural']].values\n",
    "\n",
    "# Get train and test index\n",
    "msss_splits = next(msss.split(X, Y))\n",
    "train_idx = msss_splits[0]\n",
    "val_idx = msss_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602242,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150561,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602242, 6)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = train_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150561, 6)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loan Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * K.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * K.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_16():\n",
    "    weights_path = '../models/predtrained models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    net = VGG16(include_top=False, weights=weights_path)\n",
    "    #for layer in net.layers:\n",
    "        #layer.trainable = False\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vgg_16 = vgg_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeModel:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pt_model,\n",
    "                 #loss,\n",
    "                 metrics_list,\n",
    "                 data_generator_train,\n",
    "                 data_generator_val,\n",
    "                 epochs,\n",
    "                 num_classes=6,\n",
    "                 checkpoint_path='../models/mymodels/'):\n",
    "        \n",
    "        self.pt_model = pt_model\n",
    "        #self.loss = loss\n",
    "        self.metrics_list = metrics_list\n",
    "        self.data_generator_train = data_generator_train\n",
    "        self.data_generator_val = data_generator_val\n",
    "        self.epochs = epochs\n",
    "        self.num_classes = num_classes\n",
    "        self.checkpoint_path = checkpoint_path \n",
    "        self.checkpoint = ModelCheckpoint(filepath=self.checkpoint_path,\n",
    "                                          mode=\"min\",\n",
    "                                          verbose=1,\n",
    "                                          save_best_only=True,\n",
    "                                          save_weights_only=True)\n",
    "        self.reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           factor=0.5,\n",
    "                                           patience=2,\n",
    "                                           min_lr=1e-8,\n",
    "                                            mode=\"min\")\n",
    "        self.earlystopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                                        patience=5,\n",
    "                                        mode=\"min\",\n",
    "                                        restore_best_weights=True)\n",
    "        \n",
    "    def build_model(self):\n",
    "        base_model = self.pt_model\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        any_logits = Dense(1, kernel_initializer=he_normal(seed=11))(x)\n",
    "        any_pred = Activation(\"sigmoid\", name=\"any_predictions\")(any_logits)\n",
    "        x = concatenate([any_pred, x])\n",
    "        sub_pred = Dense(self.num_classes,\n",
    "                         name=\"subtype_pred\",\n",
    "                         kernel_initializer=he_normal(seed=11),\n",
    "                         activation=\"sigmoid\")(x) \n",
    "        self.model = Model(inputs=base_model.input, outputs=[any_pred, sub_pred])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=Adam(0.0001),\n",
    "                           loss=['binary_crossentropy', focal_loss],\n",
    "                           loss_weights = [1., 0.],\n",
    "                           metrics=self.metrics_list)\n",
    "    \n",
    "    def fit_model(self):\n",
    "        return self.model.fit_generator(generator = self.data_generator_train,\n",
    "                    validation_data=self.data_generator_val,\n",
    "                    epochs=self.epochs,\n",
    "                    callbacks=[self.checkpoint,self.earlystopping],\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=-1)\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "    \n",
    "    def predict(self, data_generator_test):\n",
    "        predictions = self.model.predict_generator(data_generator_test, use_multiprocessing = True, workers=-1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor = Preprocessor(path=PATH_TRAIN_DATA,\n",
    "                                  #hu_min_value=0,\n",
    "                                  #hu_max_value=100,\n",
    "                                  augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preprocessor = Preprocessor(path=PATH_TRAIN_DATA,\n",
    "                                  #hu_min_value=0,\n",
    "                                  #hu_max_value=100,\n",
    "                                  augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = Preprocessor(path=PATH_TEST_DATA,\n",
    "                                #hu_min_value=0,\n",
    "                                #hu_max_value=100,\n",
    "                                augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n",
    "                                                train_df.iloc[train_idx], \n",
    "                                                TRAIN_BATCH_SIZE, \n",
    "                                                SHAPE,\n",
    "                                                augment = True)\n",
    "\n",
    "data_generator_val = TrainDataGenerator(train_df.iloc[val_idx], \n",
    "                                            train_df.iloc[val_idx], \n",
    "                                            VALID_BATCH_SIZE, \n",
    "                                            SHAPE,\n",
    "                                            augment = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(test_df, model):    \n",
    "    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 5, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n",
    "    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainDataGenerator' object has no attribute 'data_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-53adc2f9874a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_generator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_generator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainDataGenerator' object has no attribute 'data_ids'"
     ]
    }
   ],
   "source": [
    "train_data.loc[data_generator_train.data_ids].sum() / train_data.loc[data_generator_train.data_ids].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-cdf00718b6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel5_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-680851d59541>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     workers=-1)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2517\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/nvidia-dsd-1.3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    529\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)]..."
     ]
    }
   ],
   "source": [
    "model5 = MakeModel(pt_model=my_vgg_16,\n",
    "                      #loss=\"binary_crossentropy\", #multilabel_focal_loss(class_weights=my_class_weights, alpha=0.5, gamma=0),\n",
    "                      metrics_list={'any_predictions':['binary_crossentropy', 'accuracy'],\n",
    "                                            'subtype_pred': [focal_loss, 'accuracy']},\n",
    "                      data_generator_train = data_generator_train,\n",
    "                      data_generator_val = data_generator_val,\n",
    "                      epochs=200,\n",
    "                      num_classes=5)\n",
    "model5.build_model()\n",
    "model5.compile_model()\n",
    "model5_history = model5.fit_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions(test_df, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['any_predictions_loss']\n",
    "val_loss = model3_history.history['val_any_predictions_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Any Label Training and Validation Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['subtype_pred_loss']\n",
    "val_loss = model3_history.history['val_subtype_pred_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Subtype Label Training and Validation Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model3_history.history['subtype_pred_focal_loss']\n",
    "val_loss = model3_history.history['val_subtype_pred_focal_loss']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_loss', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_loss', color = 'orange')\n",
    "\n",
    "plt.title('Any Label Training and Validation Focal Loss by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('weighted_log_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model3_history.history['subtype_pred_accuracy']\n",
    "val_acc = model3_history.history['val_subtype_pred_accuracy']\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.plot(train_loss, label = 'training_acc', color = 'b')\n",
    "plt.plot(val_loss, label = 'val_acc', color = 'orange')\n",
    "\n",
    "plt.title('Training and Validation Subtype Accuracy by Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_any, train_proba_subtype = model3.predict(data_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_any.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba_subtype.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.concatenate((train_proba_any,train_proba_subtype),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(meta_data_df, predictions):\n",
    "    df = pd.DataFrame(predictions, columns=meta_data_df.columns, index=meta_data_df.index)\n",
    "    df = df.stack().reset_index()\n",
    "    df.loc[:, \"ID\"] = df.id.str.cat(df.subtype, sep=\"_\")\n",
    "    df = df.drop([\"id\", \"subtype\"], axis=1)\n",
    "    df = df.rename({0: \"Label\"}, axis=1)\n",
    "    df = df[['ID','Label']]\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission_5 = make_df(test_df, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_5.to_csv('../data/output/submissions/submission_model_5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
