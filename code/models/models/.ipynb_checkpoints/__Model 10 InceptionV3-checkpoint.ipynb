{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intracranial Hemorrhage Detection\n",
    "\n",
    "## Model 10: InceptionV3\n",
    "\n",
    "#### Kristina Joos\n",
    "\n",
    "---   \n",
    "\n",
    "|                 \t|                                                        \t|\n",
    "|:----------------:\t|:-------------------------------------------------------:\t|\n",
    "| Model           \t| InceptionV3                                    |\n",
    "| Augmentation      | -                                                      |\n",
    "| Windowing         | BSS Window                                                  \t|\n",
    "| Class Balancing \t| Oversampling 1:1,5 |\n",
    "| Loss Function   \t| Weighted LogLoss                                     \t|\n",
    "| Regularization  \t| Drop out (0.2)                   |\n",
    "| Epochs Run      \t| 5                                                    \t|\n",
    "| Time Run (h)   \t| 6                                         \t|\n",
    "|                 \t|                                                        \t|\n",
    "| Test Sores      \t| Loss:   0.0765                                     \t|\n",
    "| Validation      \t| Loss:                                        \t|\n",
    "| Leader Board    \t| Score: 0.39061 Rank: 403/1,345                               \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor, log\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "\n",
    "# from keras_applications.resnet import ResNet50\n",
    "from keras_applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    " \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet_50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg_16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Activation, concatenate, Dropout, MaxPooling2D, Conv2D, Flatten\n",
    "from tensorflow.keras.initializers import glorot_normal, he_normal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model, load_model, Sequential, load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_test/'\n",
    "train_images_dir = '../data/input/rsna-intracranial-hemorrhage-detection/stage_2_train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_with_correction(dcm, window_center, window_width):\n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def window_without_correction(dcm, window_center, window_width):\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bss_window(img, window):\n",
    "    brain_img = window(img, 40, 80)\n",
    "    subdural_img = window(img, 80, 200)\n",
    "    soft_img = window(img, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bss_img\n",
    "\n",
    "\n",
    "def _read(path, desired_size):\n",
    "    \n",
    "    dcm = pydicom.dcmread(path)\n",
    "    \n",
    "    try:\n",
    "        img = bss_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(desired_size)\n",
    "    \n",
    "    \n",
    "    img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
    "                 img_dir=train_images_dir, *args, **kwargs):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.indices) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            X, Y = self.__data_generation(list_IDs_temp)\n",
    "            return X, Y\n",
    "        else:\n",
    "            X = self.__data_generation(list_IDs_temp)\n",
    "            return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "\n",
    "        if self.labels is not None: \n",
    "            # Undersample\n",
    "            keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "            keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "            self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "            np.random.shuffle(self.indices)\n",
    "        else:\n",
    "            self.indices = np.arange(len(self.list_IDs))\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        if self.labels is not None: # training phase\n",
    "            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "            return X, Y\n",
    "        \n",
    "        else: # test phase\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from here https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Can be used as the loss function in model.compile()\n",
    "    ---------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "    return K.mean(out, axis=-1)\n",
    "\n",
    "\n",
    "def _normalized_weighted_average(arr, weights=None):\n",
    "    \"\"\"\n",
    "    A simple Keras implementation that mimics that of \n",
    "    numpy.average(), specifically for this competition\n",
    "    \"\"\"\n",
    "    \n",
    "    if weights is not None:\n",
    "        scl = K.sum(weights)\n",
    "        weights = K.expand_dims(weights, axis=1)\n",
    "        return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "    return K.mean(arr, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from here https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \n",
    "    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    loss = -(        y_true  * K.log(      y_pred)\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    \n",
    "    loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    \n",
    "    return K.mean(loss_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from here https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model\n",
    "def weighted_log_loss_metric(trues, preds):\n",
    "    \n",
    "    class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "    loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "    loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "\n",
    "    return - loss_samples.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from here https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model\n",
    "class PredictionCheckpoint(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_df, valid_df, \n",
    "                 test_images_dir=test_images_dir, \n",
    "                 valid_images_dir=train_images_dir, \n",
    "                 batch_size=32, input_size=(224, 224, 3)):\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_images_dir = test_images_dir\n",
    "        self.valid_images_dir = valid_images_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.test_predictions = []\n",
    "        self.valid_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self,batch, logs={}):\n",
    "        self.test_predictions.append(\n",
    "            self.model.predict_generator(\n",
    "                DataGenerator(self.test_df.index, None, self.batch_size, self.input_size, self.test_images_dir), verbose=2)[:len(self.test_df)])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model\n",
    "\n",
    "Code adapted from: https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeModel:\n",
    "    \n",
    "    def __init__(self, base_model, input_dims, batch_size=5, num_epochs=4, learning_rate=1e-3, \n",
    "                 decay_rate=1.0, decay_steps=1, weights=\"imagenet\", verbose=1):\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.weights = weights\n",
    "        self.verbose = verbose\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        \n",
    "        \n",
    "        base_model = self.base_model(\n",
    "                            include_top=False,\n",
    "                            weights=self.weights,\n",
    "                            input_shape=self.input_dims,\n",
    "                            backend = tensorflow.keras.backend, \n",
    "                            layers = tensorflow.keras.layers,\n",
    "                            models = tensorflow.keras.models,\n",
    "                            utils = tensorflow.keras.utils)\n",
    "        \n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(base_model.output) \n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Dense(keras.backend.int_shape(x)[1], activation=\"relu\", name=\"dense_hidden_1\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        out = Dense(6, activation=\"sigmoid\", name='output')(x)\n",
    "\n",
    "        self.model = keras.Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        self.model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['Accuracy', weighted_loss])\n",
    "        \n",
    "    def fit_and_predict(self, train_df, valid_df, test_df):\n",
    "\n",
    "        pred_history = PredictionCheckpoint(test_df, valid_df, input_size=self.input_dims)\n",
    "        # Adapt learning rate\n",
    "        scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: self.learning_rate * pow(self.decay_rate, floor(epoch / self.decay_steps)))\n",
    "\n",
    "        self.model.fit_generator(\n",
    "            DataGenerator(\n",
    "                train_df.index, \n",
    "                train_df, \n",
    "                self.batch_size, \n",
    "                self.input_dims, \n",
    "                train_images_dir\n",
    "            ),\n",
    "            epochs=self.num_epochs,\n",
    "            verbose=self.verbose,\n",
    "            use_multiprocessing=True,\n",
    "            workers=12,\n",
    "            callbacks=[pred_history, scheduler]\n",
    "        )\n",
    "\n",
    "        return pred_history\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_df.Label.values\n",
    "train_df = train_df.ID.str.rsplit(\"_\", n=1, expand=True)\n",
    "train_df.drop_diblicates(inplace=True)\n",
    "train_df.loc[:, \"label\"] = label\n",
    "train_df = train_df.rename({0: \"id\", 1: \"subtype\"}, axis=1)\n",
    "train_df = pd.pivot_table(train_df, index=\"id\", columns=\"subtype\", values=\"label\")\n",
    "\n",
    "label = test_df.Label.values\n",
    "test_df = test_df.ID.str.rsplit(\"_\", n=1, expand=True)\n",
    "test_df.loc[:, \"label\"] = label\n",
    "test_df = test_df.rename({0: \"id\", 1: \"subtype\"}, axis=1)\n",
    "test_df = pd.pivot_table(test_df, index=\"id\", columns=\"subtype\", values=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000000e27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000009146</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00007b8cb</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000134952</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000176f2a</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label                                                          \\\n",
       "Diagnosis      any epidural intraparenchymal intraventricular subarachnoid   \n",
       "Image                                                                        \n",
       "ID_000000e27   0.5      0.5              0.5              0.5          0.5   \n",
       "ID_000009146   0.5      0.5              0.5              0.5          0.5   \n",
       "ID_00007b8cb   0.5      0.5              0.5              0.5          0.5   \n",
       "ID_000134952   0.5      0.5              0.5              0.5          0.5   \n",
       "ID_000176f2a   0.5      0.5              0.5              0.5          0.5   \n",
       "\n",
       "                       \n",
       "Diagnosis    subdural  \n",
       "Image                  \n",
       "ID_000000e27      0.5  \n",
       "ID_000009146      0.5  \n",
       "ID_00007b8cb      0.5  \n",
       "ID_000134952      0.5  \n",
       "ID_000176f2a      0.5  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_000012eaf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_000039fa0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00005679d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_00008ce3c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_0000950d7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label                                                          \\\n",
       "Diagnosis      any epidural intraparenchymal intraventricular subarachnoid   \n",
       "Image                                                                        \n",
       "ID_000012eaf     0        0                0                0            0   \n",
       "ID_000039fa0     0        0                0                0            0   \n",
       "ID_00005679d     0        0                0                0            0   \n",
       "ID_00008ce3c     0        0                0                0            0   \n",
       "ID_0000950d7     0        0                0                0            0   \n",
       "\n",
       "                       \n",
       "Diagnosis    subdural  \n",
       "Image                  \n",
       "ID_000012eaf        0  \n",
       "ID_000039fa0        0  \n",
       "ID_00005679d        0  \n",
       "ID_00008ce3c        0  \n",
       "ID_0000950d7        0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test_Split , Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7879/7879 [==============================] - 4308s 547ms/step - loss: 0.1190 - weighted_loss: 0.1319\n",
      "Epoch 2/5\n",
      "7872/7872 [==============================] - 3197s 406ms/step - loss: 0.0939 - weighted_loss: 0.1037\n",
      "Epoch 3/5\n",
      "7869/7869 [==============================] - 3122s 397ms/step - loss: 0.0842 - weighted_loss: 0.0928\n",
      "Epoch 4/5\n",
      "7864/7864 [==============================] - 3158s 402ms/step - loss: 0.0764 - weighted_loss: 0.0841\n",
      "Epoch 5/5\n",
      "7874/7874 [==============================] - 3132s 398ms/step - loss: 0.0695 - weighted_loss: 0.0765\n"
     ]
    }
   ],
   "source": [
    "ss = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42).split(df.index)\n",
    "\n",
    "train_idx, valid_idx = next(ss)\n",
    "\n",
    "## Make Model\n",
    "model = MakeModel(base_model=InceptionV3, input_dims=(256, 256, 3), batch_size=32, learning_rate=5e-4,\n",
    "                    num_epochs=50, decay_rate=0.8, decay_steps=1, weights=\"imagenet\", verbose=1)\n",
    "\n",
    "\n",
    "history = model.fit_and_predict(df.iloc[train_idx], df.iloc[valid_idx], test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[:, :] = np.average(history.test_predictions, axis=0, weights=[0, 1, 2, 4, 6]) # let's do a weighted average for epochs (>1)\n",
    "\n",
    "test_df = test_df.stack().reset_index()\n",
    "\n",
    "test_df.insert(loc=0, column='ID', value=test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\n",
    "\n",
    "test_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
    "\n",
    "test_df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
